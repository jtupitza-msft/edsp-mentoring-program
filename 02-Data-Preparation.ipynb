{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f55ac0ac",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "Once the dataset has been profiled to gain a better understanding of its dimensionality, its quality, and the content and distribution of values in its features (columns), the next activity is to address any defects that were identified to prepare the data for exploratory data analysis (EDA) and for training and evaluating machine learning models. Depending on which defects were identified, activities may involve removing duplicate observations and deciding whether to exclude observations (rows) or features (columns) that contain missing or corrupt data, or to impute new values into those tuples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf13a77",
   "metadata": {},
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b0421b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import SimpleImputer, IterativeImputer, MissingIndicator\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038e1bb5",
   "metadata": {},
   "source": [
    "#### Import the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b167c2a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: 1309 Observations x 13 Features\n"
     ]
    }
   ],
   "source": [
    "data_dir = os.path.join(os.getcwd(), 'Data')\n",
    "source_data_file = 'titanic.csv';\n",
    "\n",
    "data_file = os.path.join(data_dir, source_data_file)\n",
    "df = pd.read_csv(data_file, header=0, index_col=0)\n",
    "\n",
    "# Ensure the index values are: seed=1, increment=1\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(f\"Shape: {df.shape[0]} Observations x {df.shape[1]} Features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c5237c",
   "metadata": {},
   "source": [
    "### 1.0. Removing Duplicate Observations\n",
    "While performing the initial profile it was determined that there were 2 more observations (rows) then there were unique values in the **name** feature. We then identified two passengers (Kate Connolly and James Kelly) with duplicate records. In each case, the first observation contained fewer missing values (NaN) so they should be kept with the second instances being excluded. For the sake of training a model this really shouldn't have any appreciable impact, but it is useful to know how to de-duplicate a dataset in situations where there could possibly be a negative impact. Also, since each passenger's *name* is certain to have no logical correlation to whether or not they survived, once the data has been de-duplicated it will be safe to exclude the **name** feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d84623ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: 1307 Observations x 12 Features\n"
     ]
    }
   ],
   "source": [
    "df.drop_duplicates(subset='name', keep='first', inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df.drop(['name'], axis=1, inplace=True)\n",
    "\n",
    "print(f\"Shape: {df.shape[0]} Observations x {df.shape[1]} Features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486565cc",
   "metadata": {},
   "source": [
    "### 2.0. Modifying the Data Type of Categorical Features Having Numerical Values\n",
    "\n",
    "While performing the initial profile features having numerical data types, that in fact contain categorical data, were detected (e.g., survived, sibsp, parch). Here those features will be converted from their present numerical types to the *Object* type to indicate their categorical nature. What's more, a new feature (survived_desc) will be created wherein descriptive labels will be mapped to the Target variable's (survived) existing numerical values. This new column will be used to enhance some visualizations that will be created while conducting exploratory data analysis (EDA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f782b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['survived_desc'] = df.survived.map({0 : 'perished', 1 : 'survived'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35d239ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "survived          object\n",
       "sex               object\n",
       "age              float64\n",
       "sibsp             object\n",
       "parch             object\n",
       "ticket            object\n",
       "fare             float64\n",
       "cabin             object\n",
       "embarked          object\n",
       "boat              object\n",
       "body             float64\n",
       "home.dest         object\n",
       "survived_desc     object\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['survived','sibsp','parch']] = df[['survived','sibsp','parch']].astype('object')\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc70323",
   "metadata": {},
   "source": [
    "### 2.0. Complete-Case Analysis *(Removing Missing Values)*\n",
    "Complete Case Analysis (CCA), also called \"list-wise deletion\" of cases, involves analyzing only those observations where all of the variables in the data set contain useful data. Subsequently, complete-case analysis consists of discarding observations where values in any of the variables are missing. Implementing CCA is prudent for the sake of avoiding any future doubt with regards to either the validity of any correlations that might be observed among the features, or the efficacy of any models that might be trained using the sample. However, in situations where the remaining sample would be too small to effectively train and test machine learning models there may be no other choice, besides sourcing additional data, than to impute missing values.\n",
    "\n",
    "#### 2.1. Quantify Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f72d17c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "survived            0\n",
       "sex                 0\n",
       "sibsp               0\n",
       "parch               0\n",
       "ticket              0\n",
       "survived_desc       0\n",
       "fare                1\n",
       "embarked            2\n",
       "age               263\n",
       "home.dest         563\n",
       "boat              821\n",
       "cabin            1012\n",
       "body             1186\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum().sort_values(ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a0494a",
   "metadata": {},
   "source": [
    "#### 2.2. Experiment with Dropping All Observations Containing Missing Values (CCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76f8594c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: 0 Observations x 13 Features\n"
     ]
    }
   ],
   "source": [
    "df_cca = df.dropna()\n",
    "print(f\"Shape: {df_cca.shape[0]} Observations x {df_cca.shape[1]} Features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a8ef67",
   "metadata": {},
   "source": [
    "#### 2.3. Experiment with Dropping Observations by Subsetting Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b6ced22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: 683 Observations x 13 Features\n"
     ]
    }
   ],
   "source": [
    "df_cca = df.dropna(subset=['survived','sex','sibsp','parch','ticket'\n",
    "                           ,'fare','embarked','age','home.dest'\n",
    "                           #,'home.dest','boat','cabin','body'\n",
    "    ])\n",
    "print(f\"Shape: {df_cca.shape[0]} Observations x {df_cca.shape[1]} Features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6aab148",
   "metadata": {},
   "source": [
    "Our experimentation has revealed that there are so many missing values that removing them all would either leave too few observations, or exclude potentially influential features. In this situation it may be necessary to experiment with various methods for imputing missing values.\n",
    "\n",
    "\n",
    "### 3.0. Imputing Missing Values\n",
    "The following sections demonstrate how to impute missing values (e.g., NaN or NULL) using two different approaches: using Pandas, and using Scikit-Learn's SimpleImputer.  The advantage of using the SimpleImputer is that is can be used to build Scikit-Learn Pipelines which both streamline and formalize the process into reusable processes called *pipelines*.\n",
    "\n",
    "The approach implemented to impute missing values is driven by the nature of the data the feature contains. The first thing to consider is whether the feature contains numerical or categorical data. If the feature is numerical then the following techniques may be most appropriate:\n",
    "- Imputing with the Mean or Median\n",
    "- Imputing with the Mode\n",
    "- Imputing with an Arbitrary Number\n",
    "- Imputing with a Value at the End of the Distribution\n",
    "- Imputing with a Random Sample\n",
    "- Imputing with MICE (Multivariate Imputation by Chained Equations)\n",
    "\n",
    "If the feature is categorical then the following techniques may be more appropriate:\n",
    "- Imputing with a Custom Category\n",
    "- Imputing with the Most Frequent Category\n",
    "- Imputing with a Missing-Value Indicator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20b19dd",
   "metadata": {},
   "source": [
    "#### Separate Numerical and Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "351120c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['age', 'fare', 'body']\n",
      "['survived', 'sex', 'sibsp', 'parch', 'ticket', 'cabin', 'embarked', 'boat', 'home.dest', 'survived_desc']\n"
     ]
    }
   ],
   "source": [
    "numerical_cols = [col for col in df.columns if df.dtypes[col] != 'O']\n",
    "categorical_cols = [col for col in df.columns if col not in numerical_cols]\n",
    "\n",
    "print(numerical_cols)\n",
    "print(categorical_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a701cc8",
   "metadata": {},
   "source": [
    "### 3.1. Imputing Numerical Values\n",
    "#### 3.1.1. Impute Missing Values with the Median or the Mean\n",
    "When imputing numerical variables a reasonable first approach would be to estimate missing values using either the **mean** of the remaining non-null observations, if the variable is reasonable parametric, or with the **median** of those remaining values if the variable contains outliers that would apply too much leverage against its appropriate mean.\n",
    "\n",
    "##### Using Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6492c5fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age     0\n",
       "fare    0\n",
       "body    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pd = df.copy() # Make a copy of the dataframe.\n",
    "\n",
    "for col in numerical_cols:\n",
    "    median = df_pd[col].median()\n",
    "    df_pd[col] = df_pd[col].fillna(median)\n",
    "    \n",
    "df_pd[numerical_cols].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea23c87a",
   "metadata": {},
   "source": [
    "##### Using Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "58e0ddeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 28.    ,  14.4542, 155.    ])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sk = df.copy() # Make a copy of the dataframe\n",
    "\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "imputer.fit(df_sk[numerical_cols])\n",
    "imputer.statistics_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a29ecf0",
   "metadata": {},
   "source": [
    "#### 3.1.2. Impute Missing Values with the Mode\n",
    "Mode imputation consists of replacing all occurrences of missing values (NA) within a variable by the mode; i.e., the most frequently occuring value.\n",
    "##### Using Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9295261a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age     0\n",
       "fare    0\n",
       "body    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pd = df.copy() # Make a copy of the dataframe.\n",
    "\n",
    "for col in numerical_cols:\n",
    "    mode = df_pd[col].mode()[0]\n",
    "    df_pd[col] = df_pd[col].fillna(mode)\n",
    "    \n",
    "df_pd[numerical_cols].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81496a1",
   "metadata": {},
   "source": [
    "##### Using Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45d931f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([24.  ,  8.05,  1.  ])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sk = df.copy() # Make a copy of the dataframe\n",
    "\n",
    "imputer = SimpleImputer(strategy='most_frequent')\n",
    "imputer.fit(df_sk[numerical_cols])\n",
    "imputer.statistics_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ed1ae4",
   "metadata": {},
   "source": [
    "#### 3.1.3. Impute Missing Values with an Arbitrary Number\n",
    "##### First, inspect the maximum value per feature to ensure the arbitrary number doesn't overlap existing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "03bcd065",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age      80.0000\n",
       "fare    512.3292\n",
       "body    328.0000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[numerical_cols].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "67901314",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age     0\n",
       "fare    0\n",
       "body    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pd = df.copy() # Make a copy of the dataframe.\n",
    "\n",
    "for col in numerical_cols:\n",
    "    df_pd[col].fillna(999, inplace=True)\n",
    "    \n",
    "df_pd[numerical_cols].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f17890e",
   "metadata": {},
   "source": [
    "##### Using Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "13614f3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([999., 999., 999.])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sk = df.copy() # Make a copy of the dataframe\n",
    "\n",
    "imputer = SimpleImputer(strategy='constant', fill_value=999)\n",
    "imputer.fit(df_sk[numerical_cols])\n",
    "imputer.statistics_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152e4e59",
   "metadata": {},
   "source": [
    "#### 3.1.4. Imputing with a Value at the End of the Distribution\n",
    "Here we will replace missing values by a value at the end of the distribution, estimated with the Gaussian approximation or the inter-quantal range proximity rule, using Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b8fb0d64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age     0\n",
       "fare    0\n",
       "body    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pd = df.copy() # Make a copy of the dataframe.\n",
    "\n",
    "for col in numerical_cols:\n",
    "    IQR = df_pd[col].quantile(0.75) - df_pd[col].quantile(0.25)\n",
    "    value = df_pd[col].quantile(0.75) + 1.25 * IQR\n",
    "    df_pd[col] = df_pd[col].fillna(value)\n",
    "    \n",
    "df_pd[numerical_cols].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1878d1",
   "metadata": {},
   "source": [
    "#### 3.1.5. Impute Missing Values with a Random Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fea353c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age     0\n",
       "fare    0\n",
       "body    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pd = df.copy() # Make a copy of the dataframe.\n",
    "\n",
    "for col in numerical_cols:\n",
    "    number_missing_values = df_pd[col].isnull().sum()\n",
    "    random_sample = df_pd[col].dropna().sample(number_missing_values, replace=True, random_state=0)\n",
    "    random_sample.index = df_pd[df_pd[col].isnull()].index\n",
    "    df_pd.loc[df_pd[col].isnull(), col] = random_sample\n",
    "    \n",
    "df_pd[numerical_cols].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76afd0eb",
   "metadata": {},
   "source": [
    "#### 3.1.6. Impute Missing Values with MICE (Multivariate Imputation by Chained Equations) Using Scikit-Learn\n",
    "The imputation techniques implemented so far have been **univariate** imputations; i.e., the values are either statitically assigned, or are estimated (calculated) using the non-null values present in the specified variable. Conversely, **multivariate** imputation techniques estimate new values taking into account the values present in all the variables (features) in the dataset. Multivariate Imputation by Chained Equations (MICE) is an imputation technique that models each variable with missing values as a function of the remaining variables and uses that estimate for imputation. Each variable with missing data can be modeled based on the remaining variable using any one of many estimators (e.g., Bayes, k-nearest neighbors, decision trees, random forests, linear regression).\n",
    "##### Using a Bayesian Ridge estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285702b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sk = df.copy() # Make a copy of the dataframe.\n",
    "\n",
    "bayes = IterativeImputer(estimator=BayesianRidge(), max_iter=10, random_state=0)\n",
    "bayes.fit(df_sk[numerical_cols])\n",
    "\n",
    "df_bayes = bayes.transform(df_sk[numerical_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9362b8",
   "metadata": {},
   "source": [
    "##### Use a K-Nearest Neighbors (KNN) estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae36d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = IterativeImputer(estimator=KNeighborsRegressor(n_neighbors=5), max_iter=10, random_state=0)\n",
    "knn.fit(df_sk[numerical_cols])\n",
    "\n",
    "df_knn = knn.transform(df_sk[numerical_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac538239",
   "metadata": {},
   "source": [
    "##### Use a Decision Tree Regressor estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f488b60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtr = IterativeImputer(estimator=DecisionTreeRegressor(max_features='sqrt', random_state=0), max_iter=10, random_state=0)\n",
    "dtr.fit(df_sk[numerical_cols])\n",
    "\n",
    "df_dtr = dtr.transform(df_sk[numerical_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59d14d3",
   "metadata": {},
   "source": [
    "##### Use an Extra Trees Regressor estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7519f5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "etr = IterativeImputer(estimator=ExtraTreesRegressor(n_estimators=10, random_state=0), max_iter=10, random_state=0)\n",
    "etr.fit(df_sk[numerical_cols])\n",
    "\n",
    "df_etr = etr.transform(df_sk[numerical_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73ba542",
   "metadata": {},
   "source": [
    "##### Plot to Compare the Performance of Each estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882c1586",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bayes = pd.DataFrame(df_bayes, numerical_cols)\n",
    "df_knn = pd.DataFrame(df_knn, numerical_cols)\n",
    "df_dtr = pd.DataFrame(df_dtr, numerical_cols)\n",
    "df_etr = pd.DataFrame(df_etr, numerical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b184b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "df.age.plot(kind='kde', ax=ax, color='black')\n",
    "df_bayes.age.plot(kind='kde', ax=ax, color='red')\n",
    "df_knn.age.plot(kind='kde', ax=ax, color='green')\n",
    "df_dtr.age.plot(kind='kde', ax=ax, color='blue')\n",
    "df_etr.age.plot(kind='kde', ax=ax, color='orange')\n",
    "\n",
    "lines, labels = ax.get_legend_handles_labels()\n",
    "labels = ['Original','Bayes','KNN','Trees','Forest']\n",
    "ax.legend(lines, labels, loc='best')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382bab8f",
   "metadata": {},
   "source": [
    "### 3.2. Imputing Categorical Values\n",
    "\n",
    "##### Inspect the percentage of missing values in each categorical variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d45149",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[categorical_cols].isnull().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52847d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "complete = categorical_cols[0:5]\n",
    "\n",
    "#y_desc = categorical_cols[-1]   # survived_desc\n",
    "\n",
    "categorical_cols.remove(complete)\n",
    "#categorical_cols.remove(y_desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a375fd",
   "metadata": {},
   "source": [
    "#### 3.2.1. Impute Missing Values with a Custom Category\n",
    "When handling missing values in Categorical features, it is customary to create an additional category by imputing the value 'Unknown', or 'Missing' into those tuples where NULL or NaN values occur.\n",
    "\n",
    "##### Using Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc944838",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pd = df.copy()\n",
    "\n",
    "for col in categorical_cols:\n",
    "    df_pd[col].fillna('Unknown', inplace=True)\n",
    "    \n",
    "df_pd[categorical_cols].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50784d7",
   "metadata": {},
   "source": [
    "##### Using Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce85c607",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sk = df.copy()\n",
    "\n",
    "imputer = SimpleImputer(strategy='constant', fill_value='Unknown')\n",
    "imputer.fit(df_sk[categorical_cols])\n",
    "\n",
    "imputer.statistics_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b02414a",
   "metadata": {},
   "source": [
    "#### 3.2.2. Impute Missing Values with the Most Frequently Category\n",
    "Most frequent category imputation consists of replacing all occurrences of missing values within a variable with the most frequently occuring value (mode).\n",
    "##### Using Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40746d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pd = df.copy()\n",
    "\n",
    "for col in categorical_cols:\n",
    "    mode = df_pd[col].mode()[0]\n",
    "    df_pd[col] = df_pd[col].fillna(mode)\n",
    "    \n",
    "df_pd[categorical_cols].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bb4136",
   "metadata": {},
   "source": [
    "##### Using Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6309a3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sk = df.copy()\n",
    "\n",
    "imputer = SimpleImputer(strategy='most_frequent')\n",
    "imputer.fit(df_sk[numerical_cols])\n",
    "imputer.statistics_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7136b5ac",
   "metadata": {},
   "source": [
    "#### 3.2.3. Impute Missing Values with a Missing-Value Indicator\n",
    "\n",
    "##### Using Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef76854c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pd = df.copy()\n",
    "\n",
    "for col in categorical_cols:\n",
    "    df_pd[col+'_NA'] = np.where(df_pd[col].isnull(), 1, 0)\n",
    "    \n",
    "df_pd.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7717843e",
   "metadata": {},
   "source": [
    "##### Using Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b62deb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sk = df.copy()\n",
    "\n",
    "indicator = MissingIndicator(error_on_new=True, features='missing-only')\n",
    "indicator.fit(df_sk)\n",
    "indicator.features_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ecf5fb",
   "metadata": {},
   "source": [
    "### 4.0. Building an Imputation Pipeine\n",
    "The previous sections have demonstrated how to implement numerous imputation methods, first using Pandas to convey the conceptual meaning of each technique, and then using various Scikit-Learn imputers that abstract the implementation details. When productionalizing a machine learning model as a service it should be assumed that new data will be submitted to that service in an unprepared condition; i.e., very similar to the 'raw' dataset provided for training the machine learning model. Therefore, that new 'dirty' data must be prepared exactly as it was during the development process. The easiest method for ensuring this is to codify each transformation (e.g., imputation, datatype change, variable-name change) in a reusable construct that accurately represents each step in the exact order as it was implemented when the training dataset was prepared.  Fortunately, Scikit-Learn (and other ML frameworks) have such a construct: the Pipeline. In this section, constructing a data preparation pipeline will be demonstrated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5e3a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify imputation method for each column\n",
    "# Nums:\n",
    "age           263 # Since age is consistantly between 21 & 39 with some outliers, Median may be appropriate\n",
    "fare            1 # Since fare is likely related to other features like Cabin, MICE may be appropriate\n",
    "body         1188 # Since body weight is likely related to other features like Sex and Age, MICE may be appropriate\n",
    "\n",
    "#Cats:\n",
    "embarked        2\n",
    "home.dest     564\n",
    "boat          823\n",
    "cabin        1014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccd9f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols_arbitrary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565707a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b3d4db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121b537c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07512f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(transformers=[('num_arbitrary', num_arbitrary, num_cols_arbitrary),\n",
    "                                              (),\n",
    "                                              (),\n",
    "                                              (),\n",
    "                                              ], remainder='passthrough')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7b8187",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor.fit(df)\n",
    "preprocessor.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c4df3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(df).isnull().sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 - AzureML",
   "language": "python",
   "name": "python38-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
